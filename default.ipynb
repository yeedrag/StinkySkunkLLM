{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw9S1uMhKeCS"
      },
      "source": [
        "We need to access OpenAI API. I use the older version, but there is a new one (1.xx), which is functionally equivalent for these purposes but with a different syntax. Feel free to modify the prompt function if you want to use the latest openai package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9e8MkMLabca",
        "outputId": "4cab448d-c1df-46d9-ea02-7e1522f77d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.28 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from openai==0.28) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from openai==0.28) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\andyw\\anaconda3\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai==0.28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNtX7mlbK2hG"
      },
      "source": [
        "We'll use pandas to have data in a nice data structure, and time to keep track of created files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UmplC-kSaZWc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from time import strftime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LDt6ltVLCmY"
      },
      "source": [
        "This is a simple function to help send and receive data from ChatGPT API. It loops up to 5 times in case OpenAI fails (which happens randomly sometimes). Here you'd also adjust the model parameters, according to the OpenAI API reference (start here: https://platform.openai.com/docs/api-reference/chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ow0RRmv-c0g_"
      },
      "outputs": [],
      "source": [
        "def prompt(Q):\n",
        "  tries = 0\n",
        "  while tries<5:\n",
        "    try:\n",
        "      response = openai.ChatCompletion.create(\n",
        "        model=mmodel,\n",
        "        messages=Q,\n",
        "        temperature=0.5,\n",
        "        #max_tokens=tkn,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        request_timeout=199\n",
        "      )['choices'][0]['message']['content']\n",
        "      break\n",
        "    except Exception as e:\n",
        "      tries = tries+1\n",
        "      print(f\"An error occurred {tries:d}th time: {e}\")\n",
        "  return(Q,response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hhwjQWdLWZz"
      },
      "source": [
        "Get time and date for convenient file naming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8a1ayoovdXxp"
      },
      "outputs": [],
      "source": [
        "dtime = strftime(\"%Y_%m_%d-%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMGnirCec-yE"
      },
      "source": [
        "Here you choose the model to use and provide the API key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L12bDXihamIV"
      },
      "outputs": [],
      "source": [
        "mmodel = \"gpt-3.5-turbo-1106\"\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX_8daefLe7F"
      },
      "source": [
        "Here we have the prompts we will be using, as well as the system prompt which servers as a general global set of instructions or information. The system prompt may be left empty.\n",
        "\n",
        "I have two prompts, one asks to generate a table, and the second one iteratively asks to continue that table. This is likely not an optimal approach, but it gives some results so a good example.\n",
        "\n",
        "I also separated the property from the prompt so that I can vary the property and keep the same prompts, and the othery way around. It does not have to be that way if more task specific prompts work better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EoRSyBE1a3WG"
      },
      "outputs": [],
      "source": [
        "PROPERTY = 'Polymer Transition Temperature'\n",
        "\n",
        "system = 'You follow instructions carefully, you provide as many rows of the table as possible, at least 50 for every prompt.'\n",
        "\n",
        "qs = [\"Provide me with a list of \"+PROPERTY+\" values for different materials. Your response should be a table consisting of 2 columns: material, value. The materials have to be typed as unique chemical compositions consisting of chemical element abbreviations and numbers only (e.g. GaAs, but not Gallium Arsenide). The values have to be single numbers, not ranges. Type out as many different values as you can. You are not allowed to type anything else than this table.\",\n",
        "      \"Continue expanding this table with new values of \"+PROPERTY+\" making sure you do not duplicate entries. Type out as many different values as you can. You are not allowed to type anything else than this table.\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGM345OOOK0t"
      },
      "source": [
        "Give the file a uniquely identifiable name. We want to save EVERYTHING or we will lose it, and each generation costs money. Either link up your google drive, or download all files each time because they are temporary (if executed on colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzAsK-xAOKTU",
        "outputId": "eb1df0dc-c404-4f50-b7b2-7c3a91b81176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving to: PolymerTransitionTemperature_2024_02_17-153632.csv and PolymerTransitionTemperature_2024_02_17-153632.txt\n"
          ]
        }
      ],
      "source": [
        "filename = PROPERTY.replace(\" \", \"\")+'_'+dtime+'.csv'\n",
        "print(f\"Saving to: {filename} and {filename.replace('csv','txt')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcorW2LDOxGw"
      },
      "source": [
        "Here we set up a structure for the chat. I like to hold the conversation in a list, and then each prompt and response inside it are dictionaries, as per OpenAI requirements. I start with just the system prompt and will append to that later.\n",
        "\n",
        "I also set up some empty lists and initial values to keep track of progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vkrmS0FMOtgc"
      },
      "outputs": [],
      "source": [
        "sss = [{\"role\": \"system\", \"content\": system}]\n",
        "tab = []\n",
        "tab_clean = []\n",
        "ur = 0\n",
        "um = 0\n",
        "i = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br38sWQCPKfn"
      },
      "source": [
        "This is the main loop that will loop over the prompts, get responses and try to put them in a structured data format.\n",
        "\n",
        "This while approach is VERY SIMPLE and does not account for many things that may be the response deom the model, so it is only an example to build upon based on the result, not a solution ready to be used. It does not even have to be used at all if one has a better/different idea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNMePOTgZ8pB",
        "outputId": "e2d997ea-de9f-4286-97b1-714f5d5cfbff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration:   1 Generated_rows: 451;     TOTAL:  Uniq_rows:   60   Uniq_materials:   60\n",
            "Stopping due to NO PROGRESS\n",
            "       Material  Value\n",
            "0     PMMA         105\n",
            "1     PS           100\n",
            "2     PVC           87\n",
            "3     PET           75\n",
            "4     PTFE         327\n",
            "..          ...    ...\n",
            "504   TPC          120\n",
            "505   TPV          120\n",
            "506   TPEO          60\n",
            "507   TPS          100\n",
            "508   TPSU         190\n",
            "\n",
            "[508 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  # here i send my first prompt first and then loop over the second one over and\n",
        "  # over again. You will likely have a different approach to this.\n",
        "  if i<1:\n",
        "    qq = qs[0]\n",
        "  else:\n",
        "    qq = qs[1]\n",
        "  # save the first prompt to the conversation\n",
        "  sss.append({\"role\": \"user\", \"content\": qq})\n",
        "  # send out the first prompt and receive the response\n",
        "  sss,ans = prompt(sss)\n",
        "  # save the response to the conversation\n",
        "  sss.append({\"role\": \"assistant\", \"content\": ans})\n",
        "\n",
        "  # we are saving the raw prompts and raw responses, in case we want to analyze\n",
        "  # or postprocess later\n",
        "  with open(filename.replace('csv','txt'), 'a') as file:\n",
        "    print(\"USER: \"+qq, file=file)\n",
        "    print(\"GPT : \"+ans, file=file)\n",
        "\n",
        "  # Here we start to grab the data into a nicer structure. For simplicity I had\n",
        "  # a lot of assumptions. I assume that the word 'value' exists in the header\n",
        "  # (see first prompt), I remove the header, the separator, and get the rest.\n",
        "  lines = ans.split('\\n')\n",
        "  if 'value' in lines[0].lower():\n",
        "    ans = '\\n'.join(lines[1:])\n",
        "  lines = ans.split('\\n')\n",
        "  if '----' in lines[0].lower():\n",
        "    ans = '\\n'.join(lines[1:])\n",
        "  lines = ans.split('\\n')\n",
        "\n",
        "  # here I try to split the table by columns. I'm assuming that they are\n",
        "  # separated with |, which is not necessary always the case.\n",
        "  tab.append(ans)\n",
        "  try:\n",
        "    for line in tab[-1].strip().split('\\n'):\n",
        "      tab_clean.append(line.strip('|').split('|'))\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  # another assumption - only two columns, \"material\" and \"value\" (see prompt)\n",
        "  # some cleanup, converting strings to numbers etc.\n",
        "  # there is no error handling or edge cases, for example 1.6-1.7 will be removed\n",
        "  # because it technically is not a number (although it kind of is).\n",
        "  df = pd.DataFrame(tab_clean, columns=['Material', 'Value'])\n",
        "  df = df[pd.to_numeric(df['Value'], errors='coerce').notna()]\n",
        "  df['Value'] = pd.to_numeric(df['Value'])\n",
        "  df.to_csv(filename, index=False)\n",
        "\n",
        "  # here I count how many new (non-duplicate) materials we are extracting each\n",
        "  # time, to monitor progress. We stop if more than 10 iterations or not progress\n",
        "  if len(df.drop_duplicates()) > ur or df['Material'].nunique() > um:\n",
        "    ur = len(df.drop_duplicates())\n",
        "    um = df['Material'].nunique()\n",
        "    i = i+1\n",
        "    if i > 10:\n",
        "      print(\"Stopping due to 10 iterations exceeded\")\n",
        "      break\n",
        "  else:\n",
        "    print(\"Stopping due to NO PROGRESS\")\n",
        "    break\n",
        "\n",
        "  print(f\"Iteration: {i:3} Generated_rows: {len(lines):3};     TOTAL:  Uniq_rows: {len(df.drop_duplicates()):4d}   Uniq_materials: {df['Material'].nunique():4d}\")\n",
        "\n",
        "print(df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
